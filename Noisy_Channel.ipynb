{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPNziwAhm3yR"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bWqQNvxV4kZ2"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "import operator\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "from collections import Counter\n",
        "import string\n",
        "unlp = spacy.blank('ur')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPR89VffXyjs",
        "outputId": "45336c54-f661-4ae3-cecf-31a62ac706fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzkZMGDNqjTM"
      },
      "source": [
        "1. Loading the whole data which acts as the corpus.\n",
        "2. Stored in Lines as sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIQ5XChcXpZ8",
        "outputId": "d2d96c38-066a-4533-d4aa-0f83a37be904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4652818\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/NLP/data.txt','r') as f:\n",
        "  Lines = f.readlines()\n",
        "\n",
        "print(len(Lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk0h_a66wxcQ"
      },
      "source": [
        "1. Removing all the \\n characters from the end of sentences\n",
        "2. Data list now contains all the tokenized words instead of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMW6daGItF9E",
        "outputId": "a7463781-9359-4c88-bc7d-de177b420064"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "for j in range(len(Lines)):\n",
        "\n",
        "  words = Lines[j].split(' ')\n",
        "  clean = []\n",
        "\n",
        "  # Removing the \\n from the last word\n",
        "  for i in words:\n",
        "    clean.append(i.strip())\n",
        "  \n",
        "  for i in clean:\n",
        "    data.append(i)\n",
        "\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vqdvS4BzjHc"
      },
      "source": [
        "No \\n characters are present anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLNciS6DmY2c",
        "outputId": "c738ad23-3e88-4543-e98a-c3f34973e367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sai', 'kha', 'ya', 'her', 'kisi', 'kay', 'bus', 'ki', 'bat', 'nhi', 'hai', 'lakin', 'main', 'ki', 'hal', 'kal', 'bi', 'aj', 'aur', 'aj']\n"
          ]
        }
      ],
      "source": [
        "print(data[0:20])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2g0i5R1qsRc"
      },
      "source": [
        "Loading mispelled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_B0TCiEmanU",
        "outputId": "4805e0dd-143d-4b72-ac57-08ae632e949b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36101\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/NLP/NLP4_i18-0562/misspellings.txt','r') as f:\n",
        "  m_data = f.readlines()\n",
        "\n",
        "print(len(m_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNxOH4q1yBv2"
      },
      "source": [
        "Storing all the correct words and their possible mistypes in the m_dict dictionary.\n",
        "1. The key is the correct word example 'ka'\n",
        "2. The values are then basically the list of mistyped words.\n",
        "\n",
        "Example: \n",
        "{'ka': ['kaz', 'cka', 'mka', 'kga', 'yka', 'kba']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wskD4rBq8m2",
        "outputId": "ad33e50c-2db6-4b8d-f232-69327e4f4c06"
      },
      "outputs": [],
      "source": [
        "m_dict = {}\n",
        "\n",
        "for j in range(len(m_data)):\n",
        "\n",
        "  if j == 0:\n",
        "    continue\n",
        "\n",
        "  clean = []\n",
        "  r_spaces = \" \".join(m_data[j].split())\n",
        "  words = r_spaces.split(' ')\n",
        "  words[0] = words[0][:-1]\n",
        "\n",
        "  for i in range(len(words)):\n",
        "    if i==0:\n",
        "      continue\n",
        "    clean.append(words[i])\n",
        "\n",
        "  m_dict[words[0]] = clean\n",
        "  \n",
        "\n",
        "print(m_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBoasomsynML"
      },
      "source": [
        "Listing words for demonstartion of 'ka'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb93WETmyaIs",
        "outputId": "babd82fa-f29d-4948-e313-47974c6ddca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['kaz', 'cka', 'mka', 'kga', 'yka', 'kba']\n"
          ]
        }
      ],
      "source": [
        "print(m_dict['ka'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhAKa-OsVvC"
      },
      "source": [
        "##Word Unigram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gqI2Va84hgZ"
      },
      "source": [
        "Returns the probabilties of all the unigrams. Also coded using dictionaries so very optimised on such a large data.\n",
        "\n",
        "From Our research paper.\n",
        "Each candidate correction, c, is scored by\n",
        "Pr(c) Pr(tlc), and then normalized by the sum\n",
        "of the scores for all proposed candidates. The\n",
        "prior, Pr(c), is estimated by\n",
        "(freq(c) + 0.5)/N, where freq(c) is the\n",
        "number of times that the word c appears in the\n",
        "1988 AP corpus (N = 44 million words))\n",
        "\n",
        "1. Here in the unigram model(prior) ,0.5 is added to the unigram occurences.\n",
        "2. This change has been catered in the code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "acS1ZXB2y8iT"
      },
      "outputs": [],
      "source": [
        "def Unigram_Probabilities(total):\n",
        "\n",
        "  u_dict = {}\n",
        "  \n",
        "  for i in total: #Iterate over the whole tokenized list\n",
        "\n",
        "    if i in u_dict:   # If word is present in dictionary keys already add a +1\n",
        "      u_dict[i]+=1\n",
        "    else:\n",
        "      u_dict[i] = 1   # This means key is being put in the first time\n",
        "\n",
        "  for key in u_dict:  # Dividing all by total vacablury for bigram formula\n",
        "    u_dict[key] = float((u_dict[key]+0.5)/len(total))  # Adding 0.5 as showed in research paper\n",
        "\n",
        "  return u_dict # Reurning the dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vbz5u5jUzkJM"
      },
      "outputs": [],
      "source": [
        "unigram_probs =  Unigram_Probabilities(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_hX1-gGfTKN"
      },
      "source": [
        "##Character unigram and bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdvuL28EAndC"
      },
      "source": [
        "Creating a tokenized list of alphabets from the file.\n",
        "1. '#' character is added before each word here.\n",
        "2. '#' is later critical for use in the discontiious function of error model calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4-jC_YKv-VRk"
      },
      "outputs": [],
      "source": [
        "char_data = []\n",
        "for i in Lines:\n",
        "  for j in i:\n",
        "    if j == '\\n':     \n",
        "      continue\n",
        "    char_data.append(j)\n",
        "    if j == ' ':\n",
        "      char_data.append('#')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv51QpunAxwX"
      },
      "source": [
        "Calculating probabilities for singel character unigram.\n",
        "1. Will be used in error model later on.\n",
        "2. Does not contain probabilty of \\n\n",
        "3. Used specificaly for calculation in insertion and substituition functions in error model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sfcnohWo_l9W"
      },
      "outputs": [],
      "source": [
        "def charUnigramProbabilities(total):\n",
        "\n",
        "  u_dict = {}\n",
        "  \n",
        "  for i in total: #Iterate over the whole tokenized list\n",
        "\n",
        "    if i in u_dict:   # If word is present in dictionary keys already add a +1\n",
        "      u_dict[i]+=1\n",
        "    else:\n",
        "      u_dict[i] = 1   # This means key is being put in the first time\n",
        "\n",
        "  for key in u_dict:  # Dividing all by total vacablury for bigram formula\n",
        "  \n",
        "    u_dict[key] = float((u_dict[key])/len(total))  # Adding 0.5 as showed in research paper\n",
        "\n",
        "  return u_dict # Reurning the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1okqM3zJ_1GU"
      },
      "outputs": [],
      "source": [
        "char_unigram_prob = charUnigramProbabilities(char_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP-LUbX_AUSD",
        "outputId": "22f93e56-1b1e-4e28-bc17-7cc504b6030f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.016241374495379317"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_unigram_prob['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac0kwO35Bufo"
      },
      "source": [
        "Bigram Probabilties for characters\n",
        "1. Used for the calculation of delete and transpose functions in error model.\n",
        "2. Plays role as the denominator.\n",
        "3. Probabilty of each bigram of letters is divided by the total occurences of the first letter in bigram.\n",
        "\n",
        "  1. For this the unigram character model above is used.\n",
        "  2. Dictionaries use for optimisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbcg_DBdBt6R",
        "outputId": "85e914ba-48e6-48a4-c636-9250554471f7"
      },
      "outputs": [],
      "source": [
        "def Bigram_Probabilty(total):\n",
        "\n",
        "  mainDict = {}  \n",
        "\n",
        "  for i in range(len(total)-1):\n",
        "\n",
        "    if (total[i],total[i+1]) in mainDict:  # If bigram is in dict,increment\n",
        "       mainDict[total[i],total[i+1]] += 1\n",
        "    else:\n",
        "      mainDict [total[i],total[i+1]] = 1   # If not initialise it with 1\n",
        "\n",
        "  #divide by unigram model\n",
        "  for g in mainDict:\n",
        "    mainDict[g] /= char_unigram_prob[g[0]]\n",
        "\n",
        "  return mainDict\n",
        "\n",
        "bigram_prob = Bigram_Probabilty(char_data)\n",
        "bigram_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOq7LExTsY-y"
      },
      "source": [
        "## Create insert, delete, substitution and transposition tables using misspellings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKrx4Oqaed4P"
      },
      "source": [
        "1. Making 2D confusion matrixes for insert,delete,reverse and substitute using python dictionaries\n",
        "2. The insert Matrix has been displayed for demonstration\n",
        "3. For start and $ for end have also been catered in the matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Xcn8cN-Dm6",
        "outputId": "6d7758e2-8a9c-499a-e1fc-22bf5dbb35bb"
      },
      "outputs": [],
      "source": [
        "# Dictionaries for error model\n",
        "d1 = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "d2 = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "d3 = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "d4 = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "\n",
        "# INSERT TABLE\n",
        "insert = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "insert['#'] = copy.deepcopy(d1)   # For start Character\n",
        "for a in insert:\n",
        "  for b in d1:\n",
        "    insert[a] = copy.deepcopy(d1)\n",
        "insert['$'] = copy.deepcopy(d1)   # For end character\n",
        "\n",
        "\n",
        "# DELETE TABLE\n",
        "delete = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "delete['#'] = copy.deepcopy(d2)   # For start Character\n",
        "for a in delete:\n",
        "  for b in d2:\n",
        "    delete[a] = copy.deepcopy(d2)\n",
        "delete['$'] = copy.deepcopy(d2)   # For end character\n",
        "\n",
        "\n",
        "# Transpose TABLE\n",
        "transpose = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "transpose['#'] = copy.deepcopy(d3)   # For start character\n",
        "for a in transpose:\n",
        "  for b in d3:\n",
        "    transpose[a] = copy.deepcopy(d3)\n",
        "transpose['$'] = copy.deepcopy(d3)   # For end character\n",
        "\n",
        "\n",
        "# SUBSTITUTE TABLE\n",
        "substitute = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "substitute['#'] = copy.deepcopy(d4)   # For start character\n",
        "for a in substitute:\n",
        "  for b in d4:\n",
        "    substitute[a] = copy.deepcopy(d4)\n",
        "substitute['$'] = copy.deepcopy(d4)   # For end character\n",
        "\n",
        "# FOR DEMONSTRATION\n",
        "print(\"Insert Matrix\")\n",
        "\n",
        "# insert['a']['b'] += 5 # Adding a value to show access\n",
        "\n",
        "for a in insert:\n",
        "  print(a,\"->\",insert[a])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQlQFR9wnA1o"
      },
      "source": [
        "ERROR MODEL CLASS\n",
        "\n",
        "1. This class will basically hold the error table probabilties.\n",
        "2. It has the 4 different matrices of insert,transpose ,delete and substitute.\n",
        "3. On calling the constructor the fillMatrices function is called autmatically using the dictionary of misspelled words.\n",
        "\n",
        "\n",
        "fillMatrices Member function()\n",
        "\n",
        "1. This functions recives a dictionary m_data.\n",
        "  1. In the dictionary the key are the correct words and the corresponding ist in the values contains all the incorrect words with edit distance one.\n",
        "  2. This dictionary is made from the misspeliings.txt shown above.\n",
        "2. Now the important part is to increment the values and for that we first have to know wether the error word is a delete,insert,transpose or substitute and then update the corresponding value of letters in the x and y axis of the respective table.\n",
        "\n",
        "3. Insert:\n",
        "  1. If the length of error word is greater then correct word then it means that an extra word was inserted and hence we move into the insert if condition.\n",
        "  2. In Insert there are basically 3 condition to look for.\n",
        "  3. If the first alphabets dont match then increment the first character with # in x axis and the mismached alphabet of the wrong word.\n",
        "  4. Finally the mismatch could also accour somewhere in between. This is detected by the loop and in case of mismathch the isert table is incremented.\n",
        "\n",
        "4. Delete:\n",
        "  Delete is very similar to insert.\n",
        "  1. The if condition here is that the error word is less then the original word.\n",
        "  2. The three condition futher are then checked accordingly and the letters are aupdated in the delete table.\n",
        "\n",
        "5. Transpose and Substituition\n",
        "  1. These two operationactually kind of overlap and can be found in a very interesting way.\n",
        "  2. The lengths of both words have to be equal in this case and so the words are compared with each other in a loop. If the letters at the same index mismatch and the next alpahbet in eqalu to the mismatched alphabet of the forst word then it means that the word woas a transpose.Also the fist letter of the second matches the second letter of the fisr word.In this case the transpose table is updated at those letters.\n",
        "  3. However if the letters mismatch and the next letters of both the words are a match or the mismatch occurs at the final letters of both words then it means that the this is a substituition and the substitute table in the error model class is updated accordingly. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "XlWs4ja8WFyG"
      },
      "outputs": [],
      "source": [
        "class Error_Model:\n",
        "\n",
        "  def __init__(self,a,b,c,d,m):\n",
        "\n",
        "    self.I = copy.deepcopy(a)\n",
        "    self.D = copy.deepcopy(b)\n",
        "    self.T = copy.deepcopy(c)\n",
        "    self.S = copy.deepcopy(d)\n",
        "\n",
        "          # Function Call to fill matrices\n",
        "    self.fillConfusionMatrices(m)\n",
        "\n",
        "\n",
        "  def fillConfusionMatrices(self,misspellings):\n",
        "\n",
        "  # i is the correct word\n",
        "  # j is the incorrect word\n",
        "\n",
        "    for i in misspellings:\n",
        "\n",
        "      L = misspellings[i]\n",
        "\n",
        "      for j in L:    # Iterating over the list of incorrect words for the correct word i\n",
        "        if len(j) > len(i):  # Insert Case\n",
        "          \n",
        "          if j[0] != i[0]:        # First Character Different\n",
        "            self.I['#'][j[0]]+=1\n",
        "\n",
        "          elif j[-1] != i[-1]:    # Last Character Different\n",
        "            self.I[j[-2]][j[-1]]+=1\n",
        "\n",
        "          else:                   # Middle Character Different\n",
        "            for k in range(len(i)):\n",
        "              if j[k] == i[k]:\n",
        "                continue\n",
        "              if j[k] != i[k]:\n",
        "                self.I[j[k-1]][j[k]]+=1\n",
        "       \n",
        "        elif len(j) < len(i): # Delete case\n",
        "\n",
        "          if j[0] != i[0]:        # First Character Different\n",
        "            self.D['#'][i[0]] += 1\n",
        "\n",
        "          elif j[-1] != i[-1]:    # Last Character Different\n",
        "            self.D[i[-2]][i[-1]] += 1\n",
        "\n",
        "          else:                   # Middle Character Different\n",
        "            for k in range(len(j)):\n",
        "              if j[k] == i[k]:\n",
        "                continue\n",
        "              if j[k] != i[k]:\n",
        "                self.D[i[k-1]][i[k]] += 1\n",
        "\n",
        "        # Transpose and substituition case\n",
        "        else:\n",
        "          for k in range(len(j)):\n",
        "            if j[k] == i[k]:\n",
        "              continue\n",
        "            elif i[k] != j[k]:         \n",
        "              if (k != (len(i)-1)):  #End condition\n",
        "                # Transpose\n",
        "                if i[k] == j[k+1] and i[k+1] == j[k]:    \n",
        "                  self.T[i[k]][i[k+1]] += 1\n",
        "                # Substitute\n",
        "                else:\n",
        "                  self.S[i[k]][j[k]] += 1\n",
        "              # Last unequal alphabet so substitute\n",
        "              else:\n",
        "                self.S[i[k]][j[k]] += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HuZjyZ4Xy5Z"
      },
      "source": [
        "1. Making an error model object providing it with empty matrices and the misspellings dictionary.\n",
        "2. The object will reciave these matrices and then populate them by default as the fill matrices function is called in the constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8ffMdGQ_ff6S"
      },
      "outputs": [],
      "source": [
        "e_model =Error_Model(insert,delete,transpose,substitute,m_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVu_wM5FuNMr",
        "outputId": "171ac1a3-908c-496b-fb66-4ad77bf744a6"
      },
      "outputs": [],
      "source": [
        "# FOR DEMONSTRATION\n",
        "print(\"Insert Matrix\")\n",
        "for a in e_model.I:\n",
        "  print(a,\"->\",e_model.I[a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "838KOXpt1U6y",
        "outputId": "53a37e61-cdb3-4cb5-ea7e-6d1cd62e79f5"
      },
      "outputs": [],
      "source": [
        "# FOR DEMONSTRATION\n",
        "print(\"Delete Matrix\")\n",
        "for a in e_model.D:\n",
        "  print(a,\"->\",e_model.D[a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkFfzjVl1Uxq",
        "outputId": "46926a5a-9bfe-4cc6-be42-d60259b01c9e"
      },
      "outputs": [],
      "source": [
        "# FOR DEMONSTRATION\n",
        "print(\"Transpose Matrix\")\n",
        "for a in e_model.T:\n",
        "  print(a,\"->\",e_model.T[a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgoSddro1Ufj",
        "outputId": "19da5287-19de-498c-d31d-d7d2f9bde6dc"
      },
      "outputs": [],
      "source": [
        "# FOR DEMONSTRATION\n",
        "print(\"Substitute Matrix\")\n",
        "for a in e_model.S:\n",
        "  print(a,\"->\",e_model.S[a])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuvjd7i5sfTx"
      },
      "source": [
        "## Creating function that calculates P(x|w) using the Error Model tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TDYaVogMH56"
      },
      "source": [
        "errorModelProb Function.\n",
        "\n",
        "1. This is the most important function of the noisy channel model as this is basically the discontiniuos function that checks which table the candidate word belongs to and then return the value of the letter changed probability from the required matrix.\n",
        "\n",
        "2. The letters to choose as the x axis and then the y axis are determined for the unique formule of each table form the discontinious function given.\n",
        "\n",
        "3. In Insert and substitute table the char unigram model is used in the denominator.\n",
        "\n",
        "4. In the delete and transpose table the bigram char model is used in the denominator.\n",
        "\n",
        "5. Checking which table the candidate belong to:\n",
        "\n",
        "  1. If length of candidate is less then error word then insert table.\n",
        "  2. If length of candidate is greate then error word then delete table.\n",
        "  3. If length of candidate word is equal then check for transpose in pairs of two.If the first pair in each word does not match and the pairs word match in cross method then use the transpose table.\n",
        "  4. If transpose is not true and two letters do not match each other but the letter after them do match then use the substituition table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "2uepPsVt6Zg6"
      },
      "outputs": [],
      "source": [
        "def errorModelProb(error_word,candidate,char_bigram,char_unigram,error_model):\n",
        "\n",
        "  # j is error word\n",
        "\n",
        "\n",
        "  ########### FOR INSERT\n",
        "\n",
        "  if len(error_word) > len(candidate):\n",
        "    if error_word[0] != candidate[0]:   # First Character Different\n",
        "      return (error_model.I['#'][error_word[0]])/1\n",
        "\n",
        "    elif error_word[-1] != candidate[-1]:    # Last Character Different\n",
        "      return (error_model.I[candidate[-2]][error_word[-1]])/char_unigram[candidate[-2]]\n",
        "\n",
        "    else:                   # Middle Character Different\n",
        "      for k in range(len(candidate)):\n",
        "        if error_word[k] == candidate[k]:\n",
        "          continue\n",
        "        if error_word[k] != candidate[k]:\n",
        "          return (error_model.I[candidate[k-1]][error_word[k]])/char_unigram[candidate[k-1]]\n",
        "\n",
        "\n",
        "    ########### FOR DELETE\n",
        "  elif len(error_word) < len(candidate):\n",
        "    if error_word[0] != candidate[0]:   # First Character Different\n",
        "          return (error_model.D['#'][candidate[0]])/1\n",
        "\n",
        "    elif error_word[-1] != candidate[-1]:    # Last Character Different\n",
        "      return (error_model.D[candidate[-2]][candidate[-1]])/char_bigram[(candidate[-2],candidate[-1])]\n",
        "\n",
        "\n",
        "    else:                   # Middle Character Different\n",
        "      for k in range(len(error_word)):\n",
        "        if candidate[k] == error_word[k]:\n",
        "          continue\n",
        "        if candidate[k] != error_word[k]:\n",
        "          return (error_model.D[candidate[k-1]][candidate[k]])/char_bigram[(candidate[k-1],candidate[k])]\n",
        "\n",
        "\n",
        "    ########### FOR DELETE AND TRANSPOSE\n",
        "\n",
        "  else:\n",
        "    for k in range(len(candidate)):\n",
        "      if candidate[k] == error_word[k]:\n",
        "        continue\n",
        "      elif candidate[k] != error_word[k]:         \n",
        "        if (k != (len(candidate)-1)):  #End condition\n",
        "          # Transpose\n",
        "          if candidate[k] == error_word[k+1] and candidate[k+1] == error_word[k]:    \n",
        "            return error_model.T[candidate[k]][candidate[k+1]] / char_bigram[(candidate[k],candidate[k+1])]\n",
        "\n",
        "          # Substitute\n",
        "          else:        \n",
        "            return error_model.S[error_word[k]][candidate[k]] / char_unigram[candidate[k]]\n",
        "\n",
        "          # Last unequal alphabet so substitute\n",
        "        else:\n",
        "            return error_model.S[error_word[k]][candidate[k]] / char_unigram[candidate[k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivWUjXXLW1qi"
      },
      "source": [
        "Demonstrating noisy channel prob with Insert case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBMkCn-gNoH8",
        "outputId": "37530413-368c-4bca-84f3-94ef98339642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.022408952886818e-06"
            ]
          },
          "execution_count": 66,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "errorModelProb('baba','abba',bigram_prob,char_unigram_prob,e_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KJx7BwjW8iW"
      },
      "source": [
        "Demonstrating noisy channel prob with Delete case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaIT8ymlWRW_",
        "outputId": "91e98198-1de1-431a-91b7-25acf68832c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "execution_count": 67,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "errorModelProb('m','um',bigram_prob,char_unigram_prob,e_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdWftUXkhjX9"
      },
      "source": [
        "Demonstrating noisy channel prob with Transpose case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW7B5sf-hjyN",
        "outputId": "eba5bf7e-fd6d-4fc0-cf4d-0bd3e9072e05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.907554898871633e-07"
            ]
          },
          "execution_count": 68,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "errorModelProb('tmu','tum',bigram_prob,char_unigram_prob,e_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me25ap2EhkGr"
      },
      "source": [
        "Demonstrating noisy channel prob with Substitute case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ohZ1veOhke8",
        "outputId": "991c34a8-a4d0-438a-82f4-210a823fe46b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1485.3580332494614"
            ]
          },
          "execution_count": 69,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "errorModelProb('gmum','ghum',bigram_prob,char_unigram_prob,e_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCFfQRQjsrvZ"
      },
      "source": [
        "##Generating Candidate Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux--eo0daPJh"
      },
      "source": [
        "For this assignment due to lack of time on my part I have used the inbuilt function of the DamerauLevenshtein distance which also checks the transpose and gives the value of transpose  and substitute as also 1.\n",
        "\n",
        "For all the other parts of assignment no inbuit functions or libraries such as NLTK are used. Everything else is made using native data structures and are explained how they are used in text boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nl-1zpi06J0",
        "outputId": "19ee7984-0006-4d46-8485-cdc52c7f9f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyxDamerauLevenshtein in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pyxDamerauLevenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V14l5HpDau7K"
      },
      "source": [
        "Show 1 distance for transpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK1jvH7Dz8N-",
        "outputId": "94ac0fd0-03c1-4dc9-ffa1-a7bc57a133fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 71,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyxdameraulevenshtein import damerau_levenshtein_distance\n",
        "\n",
        "damerau_levenshtein_distance('ka','ak')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqOx9hgmhBZa"
      },
      "source": [
        "Generate Candidate Words()\n",
        "Logic:\n",
        "1. Since the words have to be from the vocablury, all our unique words are already present in the dictianary that holds the unigram probabilties.\n",
        "2. Iterate over the keys of this dictionary and compare the edit distance of each key word with the error word.\n",
        "3. If distance is equal to 1 then add it the word to list of candidates.\n",
        "4. As a result all the candidate words will be a subset of the vocablury as required by the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5cMz49dxupO4"
      },
      "outputs": [],
      "source": [
        "def generateCandidateWords(error_word):\n",
        "\n",
        "  candidates = []   # List of candidate words\n",
        "\n",
        "  for i in unigram_probs: # Iterating unique words form vocablury\n",
        "\n",
        "    count = damerau_levenshtein_distance(error_word,i)\n",
        "\n",
        "    if count == 1:\n",
        "      candidates.append(i)\n",
        "\n",
        "  return candidates\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMgsKSvMDpkI"
      },
      "source": [
        "Generating candidate words for a demo word for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFkX5PxaDLQd",
        "outputId": "98734cef-c34e-4230-c887-16cf6366b8c6"
      },
      "outputs": [],
      "source": [
        "generateCandidateWords('baba')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-xuD6d7EHXl"
      },
      "source": [
        "##Function for finding w^ from a list of candidate words,error model and unigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyjAq_vFjYxy"
      },
      "source": [
        "correctedWord()\n",
        "\n",
        "1. This function recieves the list of candidate words,error model class and the word unigram model.\n",
        "2. After recieving these first there are checks to see if the candidate words are present or not and there checks are present.\n",
        "  \n",
        "  1. If empty display empty message and return.\n",
        "  2. If not empty display the list and proceed further. \n",
        "\n",
        "3. errorModelProb() which is basically the discontinious function for error probabilty finds out which table the candidate word belongs too and returns the noisy model probability.\n",
        "4. Once this probabilty is returned ,it is multiplied with the unigram probabilty of that candidate word.\n",
        "5. This multiplied probabilty is stored in the dictionary words with the candidate as the key and its probabibilty(( P(x|w) * P(w))) as the value.\n",
        "6. Similarly if the prob of the word is higher then the current max prob then the value of max prob is updated.\n",
        "7. Finally when all the candidate word probabilties have been calculated and stored the dictionary is looped over and the keys with the prob = max_prob are stored in max words list.\n",
        "8. This max_words list conatianng the one or more highest prob words are then displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "r4brpPT9ETyF"
      },
      "outputs": [],
      "source": [
        "def correctedWord(error,candidates,error_model,unigram_probs):\n",
        "\n",
        "  max = -1;\n",
        "  words = {}\n",
        "\n",
        "  if len(candidates) == 0:\n",
        "    print(\"No Candidate Words so no corrections for this word !!\")\n",
        "    return\n",
        "\n",
        "  print(\"Candidate Words->\")\n",
        "  print(candidates,\"\\n\")\n",
        "\n",
        "  for i in candidates:\n",
        "\n",
        "    # This part demonstrates the ( P(x|w) * P(w))\n",
        "    words[i] =  errorModelProb(error,i,bigram_prob,char_unigram_prob,error_model)* unigram_probs[i]\n",
        "\n",
        "    # Storing max probability\n",
        "    if words[i] > max:\n",
        "      max = words[i]\n",
        "\n",
        "  # For finding out argmax amongst all the candidates\n",
        "  max_words =[]\n",
        "  for j in words:\n",
        "    if words[j] == max:\n",
        "      max_words.append(j)\n",
        "\n",
        "  print(\"Corrected Words with highest Probability\")\n",
        "  print(max_words)\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvD3ZN7wHMTk"
      },
      "source": [
        "SCENARIO 1: There are possible candidate words of a error word.\n",
        "\n",
        "Here the error word is 'bataoe'\n",
        "\n",
        "The candidate words are : ['batao', 'batae', 'bataye', 'bataon', 'batane', 'batate', 'bataoge']\n",
        "\n",
        "The most corrected word from our noisy channel model is ['batao']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvtFXO1oOshf",
        "outputId": "f682bf63-cee0-4644-cd32-dc5b19574607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error Word->  bataoe \n",
            "\n",
            "Candidate Words->\n",
            "['batao', 'batae', 'bataye', 'bataon', 'batane', 'batate', 'bataoge'] \n",
            "\n",
            "Corrected Words with highest Probability\n",
            "['batao']\n"
          ]
        }
      ],
      "source": [
        "e_word = 'bataoe'\n",
        "print(\"Error Word-> \",e_word,'\\n')\n",
        "\n",
        "candidates = generateCandidateWords(e_word)\n",
        "\n",
        "correctedWord(e_word,candidates,e_model,unigram_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTwcbPsFIXt-"
      },
      "source": [
        "SCENARIO 2: There are no candidate words for the word hathoraet.\n",
        "\n",
        "A message has been printed telling that no correct assumption can be made since no candidate words were found. \n",
        "\n",
        "Caters to point 6 of the assignment where are function should handle this case as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqifHtFDk1_u",
        "outputId": "89213e4a-be8f-4f59-fe65-2b44aaa194d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error Word->  hathoraet \n",
            "\n",
            "No Candidate Words so no corrections for this word !!\n"
          ]
        }
      ],
      "source": [
        "e_word = 'hathoraet'\n",
        "print(\"Error Word-> \",e_word,'\\n')\n",
        "\n",
        "candidates = generateCandidateWords(e_word)\n",
        "\n",
        "correctedWord(e_word,candidates,e_model,unigram_probs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gPNziwAhm3yR",
        "3dhAKa-OsVvC",
        "A_hX1-gGfTKN",
        "jOq7LExTsY-y",
        "tuvjd7i5sfTx",
        "zCFfQRQjsrvZ"
      ],
      "name": "i18-0562.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
